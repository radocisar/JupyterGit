{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Last',\n",
       " 'night',\n",
       " 'I',\n",
       " 'had',\n",
       " 'a',\n",
       " 'dream',\n",
       " 'about',\n",
       " 'the',\n",
       " 'girl',\n",
       " 'who',\n",
       " 'takes',\n",
       " 'the',\n",
       " 'train,',\n",
       " 'she',\n",
       " 'was',\n",
       " 'all',\n",
       " 'alone,',\n",
       " 'the',\n",
       " 'other',\n",
       " 'ones',\n",
       " 'had',\n",
       " 'gone']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split string into list of words\n",
    "\n",
    "example_text = \"Last night I had a dream about the girl who takes the train, she was all alone, the other ones had gone\"\n",
    "words = example_text.split()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last', 'night', 'i', 'had', 'a', 'dream', 'about', 'the', 'girl', 'who']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set all lower cases\n",
    "\n",
    "lower_words = [w.lower() for w in words]\n",
    "lower_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "stopwords = nltk_stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "9\n",
      "Before\n",
      "['last', 'night', 'i', 'had', 'a', 'dream', 'about', 'the', 'girl', 'who', 'takes', 'the', 'train,', 'she', 'was', 'all', 'alone,', 'the', 'other', 'ones', 'had', 'gone']\n",
      "After\n",
      "['last', 'night', 'dream', 'girl', 'takes', 'train,', 'alone,', 'ones', 'gone']\n"
     ]
    }
   ],
   "source": [
    "print(len(set(lower_words)))\n",
    "useful_words = [word for word in lower_words if word not in stopwords]\n",
    "print(len(set(useful_words)))\n",
    "print(\"Before\")\n",
    "print(lower_words)\n",
    "print(\"After\")\n",
    "print(useful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "stemmer.stem(\"having\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['last', 'night', 'dream', 'girl', 'takes', 'train,', 'alone,', 'ones', 'gone']\n",
      "['last', 'night', 'dream', 'girl', 'take', 'train,', 'alone,', 'one', 'gone']\n"
     ]
    }
   ],
   "source": [
    "# map words to their stems\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in useful_words]\n",
    "print(useful_words[:10])\n",
    "print(stemmed_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/paolo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'having'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lem = WordNetLemmatizer()\n",
    "lem.lemmatize(\"having\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "useful ['last', 'night', 'dream', 'girl', 'takes', 'train,', 'alone,', 'ones', 'gone']\n",
      "stemmed ['last', 'night', 'dream', 'girl', 'take', 'train,', 'alone,', 'one', 'gone']\n",
      "lemmatised ['last', 'night', 'dream', 'girl', 'take', 'train,', 'alone,', 'ones', 'go']\n"
     ]
    }
   ],
   "source": [
    "lemmatised_words = [lem.lemmatize(word, 'v') for word in useful_words]\n",
    "\n",
    "print(\"useful\", useful_words[:10])\n",
    "print(\"stemmed\", stemmed_words[:10])\n",
    "print(\"lemmatised\", lemmatised_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(binary=True,\n",
    "                      stop_words='english',\n",
    "                      lowercase=True # default\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = vec.fit_transform([\"I sit on a table I bought\"])\n",
    "X_test_text = vec.transform([\"I sat on a table I will buy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28264, 8) (21895, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 731: expected 8 fields, saw 13\\nSkipping line 2836: expected 8 fields, saw 15\\nSkipping line 3058: expected 8 fields, saw 12\\nSkipping line 3113: expected 8 fields, saw 12\\nSkipping line 3194: expected 8 fields, saw 17\\nSkipping line 3205: expected 8 fields, saw 17\\nSkipping line 3255: expected 8 fields, saw 17\\nSkipping line 3520: expected 8 fields, saw 17\\nSkipping line 4078: expected 8 fields, saw 17\\nSkipping line 4087: expected 8 fields, saw 17\\nSkipping line 4088: expected 8 fields, saw 17\\nSkipping line 4499: expected 8 fields, saw 12\\n'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fin_tweets = pd.read_csv(\"/Users/paolo/Downloads/financial-tweets/stockerbot-export.csv\", error_bad_lines=False)\n",
    "\n",
    "fin_tweets_cleaned = fin_tweets.dropna().copy()\n",
    "print(fin_tweets.shape, fin_tweets_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'text', 'timestamp', 'source', 'symbols', 'company_names', 'url',\n",
       "       'verified'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_tweets_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = fin_tweets_cleaned[\"text\"]\n",
    "lower_tweets = [w.lower() for w in tweets]\n",
    "useful_tweets = [word for word in lower_tweets if word not in stopwords]\n",
    "stemmed_tweets = [stemmer.stem(word) for word in useful_tweets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discovery                             96\n",
       "The Gap                               95\n",
       "Twenty-First Century Fox              91\n",
       "Dominion Energy                       89\n",
       "HCP                                   88\n",
       "Omisego                               88\n",
       "WPX Energy                            87\n",
       "Motorola Solutions                    86\n",
       "United Parcel Service                 85\n",
       "Essex Property Trust                  85\n",
       "Ensco plc                             83\n",
       "Loews Corporation                     82\n",
       "Hilton Worldwide Holdings Inc.        82\n",
       "BlackRock                             81\n",
       "ONEOK                                 80\n",
       "Mohawk Industries                     80\n",
       "State Street Corporation              80\n",
       "Consolidated Edison                   78\n",
       "Thermo Fisher Scientific Inc.         77\n",
       "The Hershey Company                   77\n",
       "Hormel Foods Corporation              77\n",
       "DTE Energy Company                    76\n",
       "Microchip Technology Incorporated     76\n",
       "Valero Energy Corporation             76\n",
       "Southwestern Energy Company           75\n",
       "Electronic Arts Inc.                  75\n",
       "Pioneer Natural Resources Company     75\n",
       "Intuit Inc.                           75\n",
       "FireEye                               74\n",
       "Range Resources Corporation           74\n",
       "                                      ..\n",
       "ProShares UltraPro QQQ                15\n",
       "H&R Block                             14\n",
       "Raytheon Company                      14\n",
       "Dover Corporation                     13\n",
       "The Interpublic Group of Companies    13\n",
       "Ambev S.A.                            13\n",
       "Navient Corporation                   11\n",
       "CMS Energy Corporation                10\n",
       "Acuity Brands                          9\n",
       "Superior Energy Services               8\n",
       "Total System Services                  8\n",
       "Cleveland-Cliffs Inc.                  7\n",
       "Endo                                   2\n",
       "HP                                     2\n",
       "Facebook*Alphabet*Alphabet             1\n",
       "Time Warner                            1\n",
       "Oracle                                 1\n",
       "Intel*U.S.                             1\n",
       "The Goldman Sachs                      1\n",
       "eBay                                   1\n",
       "Alphabet                               1\n",
       "Macy's                                 1\n",
       "MGM Resorts                            1\n",
       "Northern                               1\n",
       " name                                  1\n",
       "CBS                                    1\n",
       "JD                                     1\n",
       "Amazon*The Gap                         1\n",
       "Harris                                 1\n",
       "American                               1\n",
       "Name: company_names, Length: 460, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = stemmed_tweets\n",
    "y = fin_tweets_cleaned[\"company_names\"]\n",
    "\n",
    "# stratify keeps the proportions\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(binary=True,\n",
    "                      stop_words='english',\n",
    "                      lowercase=True # default\n",
    "                     )\n",
    "\n",
    "# starting from our 2860 documents we took for training set, we translate them into bag of words, \n",
    "# i.e. dictionaries of word count\n",
    "X_train_text = vec.fit_transform(X_train)\n",
    "X_test_text = vec.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49992388491399"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train_text, y_train)\n",
    "y_pred = knn.predict(X_test_text)\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
